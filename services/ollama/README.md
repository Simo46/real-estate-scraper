# Ollama Service

## ğŸ¯ Descrizione
Servizio per l'integrazione con Ollama, un modello di linguaggio locale per l'analisi NLP e AI nel progetto Real Estate Scraper.

## ğŸ“ Struttura
```
services/ollama/
â”œâ”€â”€ docs/           # Documentazione del servizio
â”‚   â””â”€â”€ README.md   # Guida ai test
â”œâ”€â”€ tests/          # Test di integrazione e performance
â”‚   â”œâ”€â”€ ollama-monitor.sh
â”‚   â”œâ”€â”€ ollama-performance-monitor.js
â”‚   â”œâ”€â”€ test-ollama-integration.js
â”‚   â””â”€â”€ test-ollama-integration-complete.js
â”œâ”€â”€ Dockerfile      # Container configuration
â”œâ”€â”€ init-ollama.sh  # Script di inizializzazione
â””â”€â”€ package.json    # Dipendenze Node.js
```

## ğŸš€ Avvio rapido

### 1. Installazione dipendenze
```bash
cd services/ollama
npm install
```

### 2. Avvio servizio
```bash
# Dalla root del progetto
docker-compose up ollama
```

### 3. Test di integrazione
```bash
npm run test:integration
```

## ğŸ§ª Test disponibili

Per la guida completa ai test, consulta [`docs/README.md`](docs/README.md).

### Test principali
- **Integration Tests**: Test di connessione e integrazione
- **Performance Tests**: Monitoring performance e risorse
- **System Monitoring**: Monitoring continuo del sistema

```bash
# Esegui tutti i test
npm run test:all

# Test specifici
npm run test:integration
npm run test:performance
npm run monitor
```

## ğŸ”§ Configurazione

Il servizio Ollama Ã¨ configurato per:
- **Porta**: `11434` (standard Ollama)
- **Modelli**: Configurati via Docker
- **Integrazione**: API Gateway su porta `3000`

## ğŸ“Š Architettura

Il servizio Ollama fornisce:
- **NLP Processing**: Analisi testo e entitÃ 
- **AI Integration**: Modelli di linguaggio locali
- **Performance Monitoring**: Metriche e alerting
- **API Gateway Integration**: Endpoint RESTful

---

*Servizio autonomo con dipendenze localizzate seguendo le best practices architetturali* ğŸ—ï¸
